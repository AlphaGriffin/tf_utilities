{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "working_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import ag.logging as log\n",
    "# from PIL import Image\n",
    "log.set(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "load = np.load(\"/pub/dataset/mario/mariokart64_dataset_0.npz\")\n",
    "imgs = load['images']\n",
    "labels = load['labels']\n",
    "dataset_examples = imgs.shape[0]\n",
    "dataset_h = imgs.shape[1]\n",
    "dataset_w = imgs.shape[2]\n",
    "dataset_c = imgs.shape[3]\n",
    "dataset_classes = labels.shape[1]\n",
    "dataset_shape = imgs[0].shape\n",
    "label_example = labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "log.info(\"Images: {}\".format(imgs.shape))\n",
    "log.info(\"Labels: {}\".format(labels.shape))\n",
    "log.info(\"Image shape: {}\".format(dataset_shape))\n",
    "log.info(\"Label: {}\".format(label_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Options(): pass\n",
    "\n",
    "log_every = 500\n",
    "loss_every = 2\n",
    "\n",
    "max_out = 100   # max concurrent losing steps before bail-out\n",
    "               # note that this is only every loss_every\n",
    "\n",
    "safety_max = int(1e5)\n",
    "\n",
    "# Data Output Stuffs!!\n",
    "save_path = '/pub/models/mupen64/mariokart64/'\n",
    "all_saves = os.listdir(save_path)\n",
    "num_saves = len(all_saves)\n",
    "save_dirname = 'outputmodel{}'.format(num_saves)\n",
    "save_filename = \"mupen64_mariokart64\"\n",
    "new_path = os.path.join(save_path, save_dirname)\n",
    "if not os.path.isdir(save_dirname):\n",
    "    os.mkdir(new_path)\n",
    "    print(\"made a new dir {}\".format(new_path))\n",
    "else:\n",
    "    print(\"something..??\")\n",
    "save_path = os.path.join(new_path, save_filename)\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"InitVars\"):\n",
    "    with tf.variable_scope(\"init_vars_values\"):\n",
    "        global_step = tf.Variable(0, trainable=False, dtype='int64')\n",
    "        \n",
    "        learn_rate = tf.train.exponential_decay(0.1, global_step,\n",
    "                                                .05, 0.87,\n",
    "                                                staircase=True, name=\"Learn_decay\")\n",
    "        \n",
    "tf.add_to_collection(\"learn_rate\", learn_rate);\n",
    "tf.summary.scalar(\"learn_rate\", learn_rate);\n",
    "tf.summary.scalar(\"global_step\", learn_rate);\n",
    "tf.summary.histogram('decay_rate', learn_rate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"inputs\"):\n",
    "    Input_Tensor_Image = tf.placeholder(tf.float32, [None, \n",
    "                                                     dataset_h, \n",
    "                                                     dataset_w,\n",
    "                                                     dataset_c])\n",
    "tf.add_to_collection(\"input_tensor\", Input_Tensor_Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(Input_Tensor_Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Input_Tensor_Labels = tf.placeholder(tf.float32, [None, dataset_classes], name=\"Input_Label\")\n",
    "tf.add_to_collection(\"label\", Input_Tensor_Labels)\n",
    "Input_True_Labels = tf.argmax(Input_Tensor_Labels, dimension=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"keep_prob\"):\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"Keep_prob\")  # new feature goes with the dropout option\n",
    "tf.add_to_collection(\"keep_prob\", keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def BUILD_CONV_LAYERS(x_image, layers, f_size=5):\n",
    "    \"\"\"\n",
    "    Magically construct many Convolutional layers for a Neural Network.\n",
    "    :return:\n",
    "        An unused list of all the layers_names, weights and biases for each layer. these are added to tensorboard\n",
    "        automatically.\n",
    "    \"\"\"\n",
    "    start_shape = x_image\n",
    "    log.debug(\"Start shape= {}\".format(start_shape))\n",
    "    reducing_shape = 0\n",
    "    last_num_f = 0\n",
    "    pooling = False\n",
    "    log.info(\"Building {} Convolutional Layers\".format(layers))\n",
    "    for layer in range(layers):\n",
    "        num_f = last_num_f + (16 * (layer + 1))\n",
    "        log.debug(\"Current Num of Features= {}\".format(num_f))\n",
    "        if layer == 0:\n",
    "            img = start_shape\n",
    "            channel = dataset_c\n",
    "        else:\n",
    "            img = reducing_shape\n",
    "            channel = last_num_f\n",
    "        \n",
    "        if layer == layers -1:\n",
    "            pooling = True\n",
    "\n",
    "        last_num_f = num_f\n",
    "        log.debug(\"Start shape= {}\".format(start_shape))\n",
    "        debugs = \"New Features = {}\".format(channel)\n",
    "        reducing_shape, w, b = new_conv_layer(input=img,\n",
    "                                              filter_size=f_size,\n",
    "                                              chan=channel,\n",
    "                                              num_filters=num_f,\n",
    "                                              use_pooling=True)\n",
    "        layer_name = \"convLayer_%s\" % layer\n",
    "        log.debug(\"Finishing Layer {}\\n\\t{}\".format(layer_name, debugs))\n",
    "        #tf.summary.histogram(\"weights{}\".format(layer), w)\n",
    "        #tf.summary.histogram(\"biases{}\".format(layer), b)\n",
    "        log.info(\"Finished Layer {}\\n\\t{}\".format(layer_name, debugs))\n",
    "\n",
    "    log.info(\"Finished Building {} Conv Layers\\nMoving To Flattening...\".format(layers))\n",
    "    return reducing_shape, last_num_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def BUILD_FC_LAYER(x_image, in_features, num_fc, num_final):\n",
    "    \"\"\"\n",
    "    Magically create a huge number of Fully connected layers.\n",
    "    :param layers:\n",
    "        Number of FC layers to create.\n",
    "    :return:\n",
    "        returns a human readable non used list of the layer names. These are added to tensorboard automatically.\n",
    "    \"\"\"\n",
    "    log.info(\"Building {} Fully Connected Layers\\ninput features size = {}\\n*-\".format(num_fc, in_features))\n",
    "    # first time thru options\n",
    "    inputs = 0\n",
    "    features = 0\n",
    "    use_reLu = True\n",
    "    use_Drop = True\n",
    "    current_layer = x_image    \n",
    "    for layer in range(num_fc):        \n",
    "        if layer == 0:\n",
    "            features = in_features\n",
    "        # always set in to features and ...\n",
    "        inputs = features\n",
    "        # set features to a decay function of i\n",
    "        features = int(inputs / int((layer + 1))) # avoid a 0 zero divion error..\n",
    "        \n",
    "        if layer == num_fc - 1:  # if last time through\n",
    "            features = num_final\n",
    "            use_reLu = False  # dont use on the last time thru\n",
    "            use_Drop = False\n",
    "        \n",
    "        log.debug(\"Layer-{}\\n\\t# inputs: {}\\n\\t# outputs: {}\\n\\tuse relu? {}\\n\\tuse drop? {}\".format(\\\n",
    "            current_layer, inputs, features, use_reLu, use_Drop))\n",
    "        current_layer, w, b = new_fc_layer(input_=current_layer,\n",
    "                                           num_inputs=inputs,\n",
    "                                           num_outputs=features,\n",
    "                                           use_relu=use_reLu,\n",
    "                                           use_drop=use_Drop)\n",
    "             \n",
    "        layer_name = \"fullyLayer_{}\".format(layer)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(layer_name):\n",
    "            with tf.name_scope(\"weights\"):\n",
    "                tf.summary.histogram(\"weights\", w)\n",
    "            with tf.name_scope(\"biases\"):\n",
    "                tf.summary.histogram('biases', b)\n",
    "        log.info(\"{} : {}\".format(layer_name, current_layer))\n",
    "        \"\"\"\n",
    "    log.info(\"Finished Building {} Fully Connected Layers\".format(num_fc))\n",
    "    return current_layer # final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input_, num_inputs, num_outputs, use_relu=True, use_drop=False):\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])  # set weights\n",
    "    biases = new_biases(length=num_outputs)  # set number of OUTPUTS like give me top k or whatever...\n",
    "    layer = tf.matmul(input_, weights) + biases  # this is a #BIGMATH func\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    if use_drop:\n",
    "        layer = tf.nn.dropout(layer, keep_prob)\n",
    "    return layer, weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    \"\"\"This generates a new weight for each layer\"\"\"\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=\"weight\")\n",
    "\n",
    "def new_biases(length):\n",
    "    \"\"\"This generates a new bias for each layer\"\"\"\n",
    "    return tf.Variable(tf.constant(0.1, shape=[length]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input, filter_size, chan, num_filters, use_pooling=True):\n",
    "    log.debug(\"Building a conv layer\")\n",
    "    X_shape = [filter_size, filter_size, chan, num_filters]\n",
    "    log.debug(\"current shape: {}\".format(X_shape))\n",
    "    weights = new_weights(shape=X_shape)\n",
    "    log.debug(\"weights shape = {}\".format(weights))\n",
    "    biases = new_biases(length=num_filters)\n",
    "    log.debug(\"weights shape = {}\".format(biases))\n",
    "    \"\"\" THis is the MAGIC again... \"\"\"\n",
    "    layer = tf.nn.conv2d(input=input,  # This is the output of the last layer\n",
    "                         filter=weights,  # this is a thing\n",
    "                         strides=[1, 1, 1, 1],  # 1111 is NOT pooled MAX work\n",
    "                         padding='SAME')  # input output transformation\n",
    "    layer += biases  # Add biases\n",
    "    if use_pooling:  # this skips pixels... saves time but skips things obviously\n",
    "        log.debug(\"using pooling\")\n",
    "        layer = tf.nn.max_pool(value=layer,  # take the weights and bias together as an input\n",
    "                               ksize=[1, 2, 2, 1],  # stuff\n",
    "                               strides=[1, 2, 2, 1],  # 2 x 2 stride... could increase... check\n",
    "                               padding='SAME')  # i feel like this should already be in variable, but w/e\n",
    "    layer = tf.nn.relu(layer)  # rectified linear Unit ... like a boss\n",
    "    log.debug(\"Finished Building a conv Layer:\\n\\t{}\".format(layer))\n",
    "    return layer, weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer, num_features):\n",
    "    log.info(\"Tranitioning from Conv to fc layers\")\n",
    "    layer_shape = layer.get_shape()  # ASSERT layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "    num_ele = layer_shape[1:4].num_elements()  # like a boss...\n",
    "    #num_ele = int(num_ele/5)\n",
    "    print(num_ele)\n",
    "    num_features = num_ele\n",
    "    log.debug(\"Shape: {}, Features: {}\".format(layer.shape[:], num_features))\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])  # yep...\n",
    "    log.info(\"Finished Flattening Layers\")\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"conv_layers\"):\n",
    "    x_image, num_features = BUILD_CONV_LAYERS(Input_Tensor_Image, 7)\n",
    "log.info(\"created {} layers with {} features\".format(7, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Flat_layer\"):\n",
    "    x_image, features =flatten_layer(x_image, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"FC_layers\"):\n",
    "    final_layer = BUILD_FC_LAYER(x_image, features, num_fc=5, num_final=5)  # build FC LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(final_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(Input_Tensor_Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"sophmax\"):\n",
    "    sophmax = tf.nn.softmax(final_layer, name=\"sophmax\")\n",
    "tf.add_to_collection(\"sophmax_layer\", sophmax)\n",
    "# tf.summary.histogram('activations', sophmax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### TRAINING METHOD FROM MUPEN #### MEPHOD?\n",
    "with tf.variable_scope(\"mupen_method\"):\n",
    "    cost = tf.reduce_mean(tf.square(tf.subtract(Input_Tensor_Labels, final_layer)))\n",
    "    train_vars = tf.trainable_variables()\n",
    "#    loss = cost +\\\n",
    "#\t\t\ttf.add_n([tf.nn.l2_loss(v) for v in train_vars]) *\\\n",
    "#\t\t\t0.001\n",
    "    summ = []\n",
    "    for v in train_vars:\n",
    "        log.debug(\"v: {}\".format(v))\n",
    "        summ.append(tf.nn.l2_loss(v))\n",
    "    loss = cost + tf.add_n(summ) * 0.001\n",
    "    # loss = cost + training_vars\n",
    "tf.add_to_collection(\"loss\", loss)\n",
    "tf.summary.scalar(\"train_cost\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Dropout_Optimizer_Train'):\n",
    "    #train_drop_loss = tf.train.AdagradOptimizer(learn_rate).minimize(loss)\n",
    "    #train_drop_loss = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
    "    #train_drop_loss = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
    "    #train_drop_loss = tf.train.FtrlOptimizer(learn_rate).minimize(loss)\n",
    "    #train_drop_loss = tf.train.MomentumOptimizer(learn_rate, momentum=5).minimize(loss)\n",
    "    \n",
    "tf.add_to_collection(\"train_op\", train_drop_loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### OHHHH WE ARE GETTING CLOSE! ###\n",
    "\"\"\" Initialize the session \"\"\"\n",
    "init_op = tf.global_variables_initializer\n",
    "\n",
    "\"\"\"create summary op\"\"\"\n",
    "merged = tf.summary.merge_all\n",
    "\n",
    "\"\"\"create saver object\"\"\"\n",
    "saver = tf.train.Saver(\n",
    "#            #var_list={\"{}\".format(v): v for v in [tf.model_variables()]},\n",
    "#            write_version=tf.train.SaverDef.V2,\n",
    "#            sharded=True,\n",
    "#            keep_checkpoint_every_n_hours=.001\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Training loop variables\n",
    "epochs = 2\n",
    "batch_size = 50\n",
    "num_samples = imgs.shape[0]\n",
    "step_size = int(num_samples / batch_size) \n",
    "log.info(\"Training Run Options:\\n\\tTraining data resets: {}\\n\\tBatch size: {}\\n\\tStep Size: {}\".format(\\\n",
    "            epochs, batch_size, step_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single use for demonstration Session...\n",
    "# Start session\n",
    "#sess = tf.InteractiveSession(\"grpc://localhost:2222\")\n",
    "sess = tf.InteractiveSession()\n",
    "log.info(\"Started Session: {}\".format(sess))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "log.info(\"started Init Op\")\n",
    "log.debug(\"starting train writer: {}\".format(save_path))\n",
    "train_writer = tf.summary.FileWriter(save_path, sess.graph)\n",
    "log.info(\"train writer started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class data_prep(object):\n",
    "    def __init__(self, images, labels):\n",
    "        self.index_in_epoch = 0\n",
    "        self.num_examples = 0 \n",
    "        self.epochs_completed = 0\n",
    "        self.num_examples = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.check_data()\n",
    "        \n",
    "    def check_data(self):\n",
    "        try:\n",
    "            assert self.images.shape[0] == self.labels.shape[0]\n",
    "        except Exception as e:\n",
    "            print(\"{} is not {}\".format(self.images.shape[0], self.labels.shape[0]))\n",
    "        self.num_examples = self.images.shape[0]\n",
    "        return True\n",
    "        \n",
    "\n",
    "    def next_batch(self, batch_size, shuffle=False):\n",
    "            \"\"\" Shuffle is off by default \"\"\"\n",
    "            start = self.index_in_epoch\n",
    "            self.index_in_epoch += batch_size\n",
    "            if self.index_in_epoch > self.num_examples:\n",
    "                # Finished epoch                                                                                       \n",
    "                self.epochs_completed += 1\n",
    "                # Shuffle the data                                                                                     \n",
    "                if shuffle:\n",
    "                    perm = np.arange(self.num_examples) # should add some sort of seeding for verification            \n",
    "                    np.random.shuffle(perm)\n",
    "                    self.images = self.images[perm]\n",
    "                    self.labels = self.labels[perm]\n",
    "                # Start next epoch                                                                                     \n",
    "                start = 0\n",
    "                self.index_in_epoch = batch_size\n",
    "                assert batch_size <= self.num_examples\n",
    "            end = self.index_in_epoch\n",
    "            return self.images[start:end], self.labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batcher = data_prep(imgs, labels)\n",
    "print(batcher.images.shape[:])\n",
    "test_batch = batcher.next_batch(50)\n",
    "print(test_batch[0].shape[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Training Proceedures\n",
    "current_epoch = 0\n",
    "# while current_epoch < epochs:\n",
    "gstep = 0\n",
    "best = 1e100 # best score to beat (loss: lower is better)\n",
    "losing_streak = 0\n",
    "\n",
    "for step in range(safety_max):\n",
    "\n",
    "    # get a batch\n",
    "    batch = batcher.next_batch(42)\n",
    "\n",
    "    # common feed dict\n",
    "    feed_dict = {Input_Tensor_Image: batch[0],\n",
    "                 Input_Tensor_Labels: batch[1],\n",
    "                 keep_prob: 0.8}\n",
    "\n",
    "    # be verbose...\n",
    "    # do the step.\n",
    "    \n",
    "    #log.debug(\"train drop loss\")\n",
    "    _ = sess.run(train_drop_loss, feed_dict=feed_dict)\n",
    "    #log.info(\"train loss result: {}\".format(_))\n",
    "    \n",
    "    #log.debug(\"global_step\")\n",
    "    gstep = sess.run(global_step, feed_dict=feed_dict)\n",
    "    log.info(\"global step: {}\".format(gstep))\n",
    "    \n",
    "    #log.debug(\"learn_rate\")\n",
    "    learn = sess.run(learn_rate, feed_dict=feed_dict)\n",
    "    #log.info(\"learn rate is: {}\".format(learn))\n",
    "    \n",
    "    if step % loss_every == 0:\n",
    "        #log.debug(\"cost\")\n",
    "        new_loss = sess.run(loss, feed_dict=feed_dict)\n",
    "        log.info(\"Loss: {}\".format(new_loss))\n",
    "\n",
    "        if (new_loss < best):\n",
    "            best = new_loss\n",
    "            losing_streak = 0\n",
    "            log.info(\"# - A new High Score has been reached: {}\".format(best))\n",
    "            \n",
    "            log.info(\"saving...\")\n",
    "            saver.save(sess, save_path, global_step=gstep)\n",
    "            \n",
    "        else: # didn't beat high score\n",
    "            log.debug(\"   losing... {}\".format(losing_streak))\n",
    "            losing_streak += 1\n",
    "            \n",
    "            if losing_streak >= max_out:\n",
    "                log.info(\"bailing out for no recent progress!\")\n",
    "                break\n",
    "    \n",
    "    if step % log_every == 0:    \n",
    "        log.debug(\"merging\")\n",
    "        #log.debug(\"merged is: {}\".format(merged))\n",
    "        summary = sess.run(merged(), feed_dict=feed_dict)\n",
    "        log.debug(\"summary size: {}\".format(len(summary)))\n",
    "        \n",
    "        log.debug(\"adding to tensorboard summary\")\n",
    "        train_writer.add_summary(summary, step)\n",
    "\n",
    "\n",
    "    global_step += 1\n",
    "\n",
    "log.debug(\"final save\")\n",
    "saver.save(sess, save_path, global_step=gstep)\n",
    "\n",
    "print(\"finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
