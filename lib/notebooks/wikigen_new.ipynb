{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# wikipedia stuff\n",
    "import os, sys, datetime, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import ag.logging as log\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from tqdm import tqdm\n",
    "buf=\"$\" * 40\n",
    "log.set(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Number of Staring Characters: 1288556\n",
      "# Number of Working Characters: 322139\n"
     ]
    }
   ],
   "source": [
    "# open dataset\n",
    "class dataset(): pass\n",
    "dataset.text = open('/pub/dataset/wiki/wiki.test.raw', encoding='utf-8', errors=\"surrogateescape\").read()\n",
    "dataset.len = len(dataset.text)\n",
    "print(\"# Number of Staring Characters: {}\".format(dataset.len))\n",
    "# oh NOOOO!!!! we have to trunkate or dataset in a weird place... this could hoze our results!\n",
    "dataset.text = dataset.text[:int(dataset.len/4)] # gotta cut it by 80%\n",
    "dataset.len = len(dataset.text)\n",
    "print(\"# Number of Working Characters: {}\".format(dataset.len))\n",
    "dataset.sample = dataset.text[:100]\n",
    "dataset.chars = sorted(list(set(dataset.text)))\n",
    "dataset.len_chars = len(dataset.chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Number of Character: 322139\n",
      "# Number of Unique Characters: 155\n",
      "# Sample:  \n",
      " = Robert Boulter = \n",
      " \n",
      " Robert Boulter is an English film , television and theatre actor . He had \n",
      "\n",
      "# All Unique Characters:\n",
      " ['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '©', '°', 'É', 'à', 'á', 'ã', 'ä', 'æ', 'è', 'é', 'ê', 'í', 'ñ', 'ó', 'ö', 'ü', 'ě', 'ī', 'Ō', 'ō', 'ū', 'ǐ', 'ǜ', 'ə', 'έ', 'δ', 'ε', 'ι', 'λ', 'μ', 'ν', 'ο', 'π', 'ς', 'σ', 'τ', 'υ', 'ό', '‑', '–', '—', '’', '−', '♯', '伊', '傳', '八', '勢', '史', '型', '士', '大', '律', '成', '戦', '春', '望', '杜', '東', '甫', '甲', '聖', '艦', '處', '衛', '解', '詩', '贈', '邵', '鉄', '集']\n"
     ]
    }
   ],
   "source": [
    "print(\"# Number of Character: {}\".format(dataset.len))\n",
    "print(\"# Number of Unique Characters: {}\".format(dataset.len_chars))\n",
    "print(\"# Sample: {}\\n\".format(dataset.sample))\n",
    "print(\"# All Unique Characters:\\n {}\".format(dataset.chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "char2id = dict((c, i) for i, c in enumerate(dataset.chars))\n",
    "id2char = dict((i, c) for i, c in enumerate(dataset.chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# probibilty of each next character\n",
    "def sample(pred):\n",
    "    r = random.uniform(0,1)\n",
    "    s= 0\n",
    "    l = len(pred)\n",
    "    char_id =  l-1 \n",
    "    for i in range(l):\n",
    "        s += pred[i]\n",
    "        if s >= r:\n",
    "            char_id = i\n",
    "            break\n",
    "    \n",
    "    one_hot_char = np.zeros(shape=[dataset.len_chars])\n",
    "    one_hot_char[char_id] = 1.0\n",
    "    return one_hot_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# batch and options\n",
    "class options(): pass\n",
    "options.batch_section = 50\n",
    "options.skip = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Section: 161045\n"
     ]
    }
   ],
   "source": [
    "# do some work... create the sections list\n",
    "dataset.sections = []\n",
    "dataset.next_chars = []\n",
    "for i in range(0, dataset.len - options.batch_section, options.skip):\n",
    "    dataset.sections.append(dataset.text[i: i + options.batch_section])\n",
    "    dataset.next_chars.append(dataset.text[i + options.batch_section])\n",
    "dataset.len_sections = len(dataset.sections)\n",
    "print(\"Length of Section: {}\".format(dataset.len_sections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# if you want to explore your memory size... go for /3\n",
    "# x = np.zeros((int(644253/4), int(50), 159))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create input data sctructure placeholders\n",
    "tensor_inputs = np.zeros((dataset.len_sections, options.batch_section, dataset.len_chars))\n",
    "tensor_labels = np.zeros((dataset.len_sections, dataset.len_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of labels:\t161045\n",
      "Labels: [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# for each_position and each batch of 50, one at a time, in all batches with index.\n",
    "for i, section in enumerate(dataset.sections):\n",
    "    # for this poistion and this character in this section with its index\n",
    "    for j, char in enumerate(section):\n",
    "        # place a datapoint at for T at poistion 4000 h is the next char\n",
    "        tensor_inputs[i,j,char2id[char]] = 1\n",
    "    tensor_labels[i, char2id[dataset.next_chars[i]]] = 1\n",
    "print(\"Len of labels:\\t{}\\nLabels: {}\".format(len(tensor_labels),tensor_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### BOOMSKI here we go... data wash complete.\n",
    "options.batch_size = 512\n",
    "options.iters = 1e4\n",
    "options.log_every = 50\n",
    "options.save_every = 100\n",
    "options.hidden_nodes = 1024\n",
    "options.max_text = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save path is: /pub/dataset/wiki/log/perm\n"
     ]
    }
   ],
   "source": [
    "starting_text = \"The world is filled with music.\"\n",
    "if True:\n",
    "    save_path = \"/pub/dataset/wiki/log/perm\"\n",
    "    if os.path.isdir(save_path):\n",
    "        pass\n",
    "    else:\n",
    "        \"had to make the dir.\"\n",
    "        tf.gfile.MakeDirs(save_path)\n",
    "else:    \n",
    "    save_path = \"/home/eric/datasets/wiki/log\"\n",
    "    if os.path.isdir(save_path):\n",
    "        print(\"check\")\n",
    "        all_logs = os.listdir(save_path)\n",
    "        num_logs = len(all_logs)\n",
    "        new_path = os.path.join(save_path,\"wikigen_{}\".format(num_logs))\n",
    "        tf.gfile.MakeDirs(new_path)\n",
    "        if os.path.isdir(new_path):\n",
    "            save_path = new_path\n",
    "        else:\n",
    "            print(\"error creating folder\")\n",
    "\n",
    "    else:\n",
    "        print(\"errors finding path\")\n",
    "print(\"Save path is: {}\".format(save_path))\n",
    "save_filename = \"ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 161045\n",
      "steps per epoch: 314\n"
     ]
    }
   ],
   "source": [
    "print(\"training data size: {}\".format(len(tensor_inputs)))\n",
    "print(\"steps per epoch: {}\".format(int(len(tensor_inputs)/options.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"inputs\"):\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # global_step = tf.Variable('global_step',trainable=False)\n",
    "    learn_rate = tf.train.exponential_decay(\n",
    "            0.1, global_step,\n",
    "            1e-7, 0.87, staircase=True,\n",
    "            name=\"Learn_decay\")\n",
    "\n",
    "    #tf.add_to_collection(\"learn_rate\", learn_rate)\n",
    "    #tf.summary.scalar(\"learn_rate\", learn_rate)\n",
    "    # input placeholders still inside of inputs scope\n",
    "    input_holder = tf.placeholder(tf.float32, \n",
    "                                  [options.batch_size,\n",
    "                                   options.batch_section,\n",
    "                                   dataset.len_chars],\n",
    "                                  name=\"Input_Tensor\")\n",
    "\n",
    "    input_label = tf.placeholder(tf.float32, \n",
    "                                 [options.batch_size,\n",
    "                                  dataset.len_chars],\n",
    "                                 name=\"Input_label\")\n",
    "#tf.add_to_collection(\"input_tensor\", input_holder)\n",
    "#tf.add_to_collection(\"input_label\", input_label)\n",
    "# input game, out game, forget gate, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"gates\"):\n",
    "    #'''input gate'''\n",
    "    # weights for input\n",
    "    w_ii = tf.Variable(tf.truncated_normal([dataset.len_chars,\n",
    "                                           options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"in_weights\")\n",
    "    # weights for previous output\n",
    "    w_io = tf.Variable(tf.truncated_normal([options.hidden_nodes,\n",
    "                                            options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"previous_weights\")\n",
    "    # bias\n",
    "    b_i = tf.Variable(tf.zeros([1, options.hidden_nodes]),\n",
    "                      name=\"input_bias\")\n",
    "    # for re optimizing\n",
    "    #tf.add_to_collection(\"in_weights\", w_ii)\n",
    "    #tf.add_to_collection(\"previous_weights\", w_io)\n",
    "    #tf.add_to_collection(\"input_bias\", b_i)\n",
    "\n",
    "    #'''forget gate'''\n",
    "    # weights for input\n",
    "    w_fi = tf.Variable(tf.truncated_normal([dataset.len_chars,\n",
    "                                           options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"forget_weights\")\n",
    "    # weights for previous output\n",
    "    w_fo = tf.Variable(tf.truncated_normal([options.hidden_nodes,\n",
    "                                            options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"forget_previous_weights\")\n",
    "    # bias\n",
    "    b_f = tf.Variable(tf.zeros([1, options.hidden_nodes]),\n",
    "                      name=\"forget_bias\")\n",
    "    # for re optimizing\n",
    "    #tf.add_to_collection(\"forget_weights\", w_fi)\n",
    "    #tf.add_to_collection(\"forget_previous_weights\", w_fo)\n",
    "    #tf.add_to_collection(\"forget_bias\", b_f)\n",
    "\n",
    "    #'''output gate'''\n",
    "    # weights for input\n",
    "    w_oi = tf.Variable(tf.truncated_normal([dataset.len_chars,\n",
    "                                           options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"out_weights\")\n",
    "    # weights for previous output\n",
    "    w_oo = tf.Variable(tf.truncated_normal([options.hidden_nodes,\n",
    "                                            options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"out_previous_weights\")\n",
    "    # bias\n",
    "    b_o = tf.Variable(tf.zeros([1, options.hidden_nodes]),\n",
    "                      name=\"output_bias\")\n",
    "    # for re optimizing\n",
    "    #tf.add_to_collection(\"output_weights\", w_oi)\n",
    "    #tf.add_to_collection(\"out_previous_weights\", w_oo)\n",
    "    #tf.add_to_collection(\"output_bias\", b_o)\n",
    "\n",
    "    #'''memory gate'''\n",
    "    # weights for input\n",
    "    w_ci = tf.Variable(tf.truncated_normal([dataset.len_chars,\n",
    "                                           options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"memory_weights\")\n",
    "    # weights for previous output\n",
    "    w_co = tf.Variable(tf.truncated_normal([options.hidden_nodes,\n",
    "                                            options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"memory_previous_weights\")\n",
    "    # bias\n",
    "    b_c = tf.Variable(tf.zeros([1, options.hidden_nodes]),\n",
    "                      name=\"memory_bias\")\n",
    "    # for re optimizing\n",
    "    #tf.add_to_collection(\"memory_weights\", w_ci)\n",
    "    #tf.add_to_collection(\"memory_previous_weights\", w_co)\n",
    "    #tf.add_to_collection(\"memory_bias\", b_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(i, o, state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "    #(input * forget weights) + (output * weights for previous output) + bias\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "    #(input * output weights) + (output * weights for previous output) + bias\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "    #(input * internal state weights) + (output * weights for previous output) + bias\n",
    "    memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "\n",
    "    #...now! multiply forget gate * given state    +  input gate * hidden state\n",
    "    state = forget_gate * state + input_gate * memory_cell\n",
    "    #squash that state with tanh nonlin (Computes hyperbolic tangent of x element-wise)\n",
    "    #multiply by output\n",
    "    output = output_gate * tf.tanh(state)\n",
    "    return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### OHHHH WE ARE GETTING CLOSE! ###\n",
    "# \"\"\" Initialize the session \"\"\"\n",
    "# init_op = tf.global_variables_initializer\n",
    "# \n",
    "# \"\"\"create summary op\"\"\"\n",
    "# merged = tf.summary.merge_all\n",
    "# \n",
    "# \"\"\"create saver object\"\"\"\n",
    "# saver = tf.train.Saver(\n",
    "#             #var_list={\"{}\".format(v): v for v in [tf.model_variables()]},\n",
    "#             write_version=tf.train.SaverDef.V2,\n",
    "#             sharded=True,\n",
    "#             keep_checkpoint_every_n_hours=.001\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros:0\", shape=(512, 1024), dtype=float32) Tensor(\"zeros_1:0\", shape=(512, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output = tf.zeros([options.batch_size, options.hidden_nodes])\n",
    "state = tf.zeros([options.batch_size, options.hidden_nodes])\n",
    "print(output, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working first set\n",
      "working final set\n",
      "process complete\n"
     ]
    }
   ],
   "source": [
    "###... still chopping wood...\n",
    "outputs_all_i = []\n",
    "labels_all_i = []\n",
    "for i in range(options.batch_section):\n",
    "    #calculate state and output from LSTM\n",
    "    output, state = lstm_cell(input_holder[:, i, :], output, state)\n",
    "    #to start, \n",
    "    if i == 0:\n",
    "        print(\"working first set\")\n",
    "        #store initial output and labels\n",
    "        outputs_all_i = output\n",
    "        labels_all_i = input_holder[:, i+1, :]\n",
    "    #for each new set, concat outputs and labels\n",
    "    if i != options.batch_section - 1:\n",
    "        # print(\"working set {}\".format(i+1))\n",
    "        #concatenates (combines) vectors along a dimension axis, not multiply\n",
    "        outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "        labels_all_i = tf.concat([labels_all_i, input_holder[:, i+1, :]], 0)\n",
    "    else:\n",
    "        #final store\n",
    "        print(\"working final set\")\n",
    "        outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "        labels_all_i = tf.concat([labels_all_i, input_label], 0)\n",
    "print(\"process complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Classificator\n",
    "with tf.variable_scope(\"superNet_layer\"):\n",
    "    sw = tf.Variable(tf.truncated_normal([options.hidden_nodes, dataset.len_chars], -0.1,0.1),\n",
    "                    name=\"superNet_weights\")\n",
    "    sb = tf.Variable(tf.zeros(dataset.len_chars),name=\"superNet_biases\")\n",
    "tf.summary.histogram('superNet_weights', sw); \n",
    "tf.summary.histogram('superNet_biases', sb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# with tf.device(\"gpu0\"):\n",
    "with tf.variable_scope(\"training_op\"):\n",
    "    # google like the word logits.. it represents the \"final Layer\"\n",
    "    final_layer = tf.matmul(outputs_all_i, sw) + sb\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_all_i, logits=final_layer))\n",
    "    tf.add_to_collection(\"softmax_activations\", cost);\n",
    "    tf.summary.scalar(\"softmax_activations\", cost);\n",
    "    tf.summary.histogram('softmax_activations', cost);\n",
    "    # should have them all here lined up... but im not there yet...\n",
    "    train_op = tf.train.AdagradOptimizer(learn_rate).minimize(cost, global_step=global_step)\n",
    "    train_op = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "    tf.add_to_collection(\"train_op\", train_op);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_data = tf.placeholder(tf.float32, shape=[1, dataset.len_chars])\n",
    "test_output = tf.Variable(tf.zeros([1, options.hidden_nodes]))\n",
    "test_state = tf.Variable(tf.zeros([1, options.hidden_nodes]))\n",
    "\n",
    "#Reset at the beginning of each test\n",
    "reset_test_state = tf.group(test_output.assign(tf.zeros([1, options.hidden_nodes])), \n",
    "                            test_state.assign(tf.zeros([1, options.hidden_nodes])))\n",
    "\n",
    "#LSTM\n",
    "test_output, test_state = lstm_cell(test_data, test_output, test_state)\n",
    "with tf.variable_scope(\"test_op\"):\n",
    "    test_op = tf.nn.softmax(tf.matmul(test_output, sw) + sb)\n",
    "    tf.add_to_collection(\"test_softmax_activations\", test_op);\n",
    "    tf.summary.scalar(\"test_softmax_activations\", test_op);\n",
    "    tf.summary.histogram('test_softmax_activations', test_op);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged: Tensor(\"Merge/MergeSummary:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Initialize the session \"\"\"\n",
    "init_op = tf.global_variables_initializer\n",
    "\n",
    "\"\"\"create summary op\"\"\"\n",
    "merged = tf.summary.merge_all()\n",
    "print(\"merged: {}\".format(merged))\n",
    "\n",
    "\"\"\"create saver object\"\"\"\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *I* Starting the wikigen Training!\n",
      " ~D~ session started\n",
      " ~D~ init started\n",
      " *I* Training for 1 iters\n"
     ]
    }
   ],
   "source": [
    "log.info(\"Starting the wikigen Training!\")\n",
    "#with tf.Graph().as_default():\n",
    "try:\n",
    "    if sess:\n",
    "        log.debug(\"closing old sess\")\n",
    "        sess.close()\n",
    "except: pass\n",
    "sess = tf.InteractiveSession()\n",
    "log.debug(\"session started\")\n",
    "#init graph, load model\n",
    "sess.run(init_op())\n",
    "train_writer = tf.summary.FileWriter(save_path, sess.graph)\n",
    "# test_writer = tf.summary.FileWriter(self.options.logDir, sess.graph)\n",
    "log.debug(\"init started\")\n",
    "offset = 0\n",
    "iters = 1  # testing\n",
    "log.info(\"Training for {} iters\".format(iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\r",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the Training for 10000.0 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 30/30 [00:00<00:00, 458.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/144 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|███       | 44/144 [00:00<00:00, 439.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|██████    | 88/144 [00:00<00:00, 438.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|█████████▍| 136/144 [00:00<00:00, 443.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 144/144 [00:00<00:00, 439.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 1/10000 [00:11<32:10:15, 11.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 2/10000 [00:21<30:57:25, 11.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 3/10000 [00:32<30:23:00, 10.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 4/10000 [00:41<29:23:56, 10.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 5/10000 [00:51<28:35:08, 10.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 6/10000 [01:01<28:12:54, 10.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 7/10000 [01:11<27:53:31, 10.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 8/10000 [01:20<27:30:59,  9.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 9/10000 [01:30<27:14:17,  9.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 10/10000 [01:40<27:12:13,  9.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 11/10000 [01:49<27:03:42,  9.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 12/10000 [01:59<26:57:50,  9.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 13/10000 [02:09<26:59:04,  9.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 14/10000 [02:18<26:53:38,  9.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 15/10000 [02:28<26:48:23,  9.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 16/10000 [02:37<26:45:13,  9.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 17/10000 [02:47<26:42:13,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 18/10000 [02:57<26:40:00,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 19/10000 [03:06<26:39:09,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 20/10000 [03:16<26:41:44,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 21/10000 [03:26<26:40:11,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 22/10000 [03:35<26:39:59,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 23/10000 [03:45<26:38:09,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 24/10000 [03:54<26:38:19,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 25/10000 [04:04<26:39:23,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 26/10000 [04:14<26:38:05,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 27/10000 [04:23<26:37:38,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 28/10000 [04:33<26:36:35,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 29/10000 [04:42<26:35:32,  9.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 30/10000 [04:52<26:35:06,  9.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 31/10000 [05:02<26:34:03,  9.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 32/10000 [05:11<26:33:14,  9.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 33/10000 [05:21<26:45:50,  9.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 34/10000 [05:31<26:43:08,  9.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 35/10000 [05:40<26:40:30,  9.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 36/10000 [05:50<26:38:49,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 37/10000 [05:59<26:38:39,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 38/10000 [06:09<26:36:34,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 39/10000 [06:19<26:35:41,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 40/10000 [06:28<26:35:31,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 41/10000 [06:38<26:34:16,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 42/10000 [06:48<26:42:22,  9.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 43/10000 [06:57<26:39:47,  9.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 44/10000 [07:07<26:37:14,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 45/10000 [07:16<26:35:06,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 46/10000 [07:26<26:33:07,  9.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 47/10000 [07:36<26:32:44,  9.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 48/10000 [07:45<26:31:50,  9.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 49/10000 [07:55<26:34:26,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 50/10000 [08:04<26:34:49,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 51/10000 [08:14<26:34:45,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 52/10000 [08:24<26:46:06,  9.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 53/10000 [08:34<26:59:18,  9.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 54/10000 [08:43<26:49:48,  9.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 55/10000 [08:53<26:44:13,  9.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 56/10000 [09:03<26:39:45,  9.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 57/10000 [09:12<26:35:34,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 58/10000 [09:22<26:33:55,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 59/10000 [09:31<26:33:11,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 60/10000 [09:41<26:40:42,  9.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 61/10000 [09:51<26:37:15,  9.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 62/10000 [10:00<26:36:15,  9.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 63/10000 [10:10<26:33:51,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 64/10000 [10:20<26:37:27,  9.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 65/10000 [10:29<26:34:58,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 66/10000 [10:39<26:33:54,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 67/10000 [10:48<26:31:59,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 68/10000 [10:58<26:30:24,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 69/10000 [11:08<26:33:26,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 70/10000 [11:17<26:31:53,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 71/10000 [11:27<26:31:12,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 72/10000 [11:37<26:30:16,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 73/10000 [11:46<26:30:48,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 74/10000 [11:56<26:30:03,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 75/10000 [12:05<26:30:46,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 76/10000 [12:15<26:29:05,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 77/10000 [12:25<26:28:19,  9.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 78/10000 [12:34<26:28:17,  9.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 79/10000 [12:44<26:27:37,  9.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 80/10000 [12:54<26:35:47,  9.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 81/10000 [13:03<26:32:39,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 82/10000 [13:13<26:31:20,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 83/10000 [13:22<26:31:18,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 84/10000 [13:32<26:30:05,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 85/10000 [13:42<26:28:23,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 86/10000 [13:51<26:27:13,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 87/10000 [14:01<26:36:54,  9.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 88/10000 [14:11<26:33:03,  9.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 89/10000 [14:20<26:31:02,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 90/10000 [14:30<26:28:47,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 91/10000 [14:39<26:29:29,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 92/10000 [14:49<26:28:05,  9.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 93/10000 [14:59<26:27:18,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 94/10000 [15:08<26:26:08,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 95/10000 [15:18<26:25:40,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 96/10000 [15:27<26:26:04,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 97/10000 [15:37<26:25:49,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 98/10000 [15:47<26:25:37,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 99/10000 [15:56<26:24:55,  9.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 100/10000 [16:06<26:25:03,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 30/30 [00:00<00:00, 470.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/144 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▍      | 50/144 [00:00<00:00, 493.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████▉   | 99/144 [00:00<00:00, 487.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 144/144 [00:00<00:00, 484.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 101/10000 [16:17<27:55:02, 10.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 102/10000 [16:27<27:27:59,  9.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 103/10000 [16:37<27:09:04,  9.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 104/10000 [16:46<26:54:20,  9.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 105/10000 [16:56<26:45:24,  9.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 106/10000 [17:05<26:42:09,  9.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 107/10000 [17:15<26:36:32,  9.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 108/10000 [17:25<26:39:05,  9.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 109/10000 [17:34<26:34:47,  9.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 110/10000 [17:44<26:29:51,  9.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 111/10000 [17:54<26:27:31,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 112/10000 [18:03<26:37:08,  9.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 113/10000 [18:13<26:32:21,  9.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 114/10000 [18:23<26:29:54,  9.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 115/10000 [18:32<26:30:00,  9.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 116/10000 [18:42<26:47:07,  9.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 117/10000 [18:55<28:57:16, 10.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 118/10000 [30:25<588:24:08, 214.35s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "x = options.iters\n",
    "\n",
    "msg = \"Starting the Training for {} iterations\".format(x)\n",
    "print(msg)\n",
    "for step in tqdm(range(int(x))):\n",
    "    global_step += 1\n",
    "    offset = offset % len(tensor_inputs)\n",
    "    if offset <= (len(tensor_inputs) - options.batch_size):\n",
    "        batch_data = tensor_inputs[offset: offset + options.batch_size]\n",
    "        batch_labels = tensor_labels[offset: offset + options.batch_size]\n",
    "        offset += options.batch_size\n",
    "    else:\n",
    "        to_add = options.batch_size - (len(tensor_inputs) - offset)\n",
    "        batch_data = np.concatenate((tensor_inputs[offset: len(tensor_inputs)], tensor_inputs[0: to_add]))\n",
    "        batch_labels = np.concatenate((tensor_labels[offset: len(tensor_inputs)], tensor_labels[0: to_add]))\n",
    "        offset = to_add\n",
    "    # this is finally it ... GEEZEEZEZ\n",
    "    feed_dict={input_holder: batch_data, input_label: batch_labels}\n",
    "    _, loss_, global_step_, learn  = sess.run([train_op,cost, global_step, learn_rate], feed_dict=feed_dict)\n",
    "    \n",
    "    if step % options.log_every == 0:        \n",
    "        msg = \"{} :: Current Step: {}\\n\\t-Loss: {}\\n\\t-Learn rate: {}\\n{}\\n\".format(datetime.time(), \n",
    "                                                                                    global_step_, \n",
    "                                                                                    cost, \n",
    "                                                                                    learn, \n",
    "                                                                                    buf)\n",
    "        \n",
    "        if step % options.save_every == 0:\n",
    "            reset_test_state.run()\n",
    "            test_generated = starting_text\n",
    "            # log.debug(\"in a testing phase\")\n",
    "            for i in tqdm(range(len(starting_text) - 1)):\n",
    "                test_X = np.zeros((1, dataset.len_chars))\n",
    "                test_X[0, char2id[starting_text[i]]] = 1.\n",
    "                test_feed={test_data: test_X}\n",
    "                _ = sess.run(test_op, feed_dict=test_feed)\n",
    "            # test generated\n",
    "            test_X = np.zeros((1, dataset.len_chars))\n",
    "            test_X[0, char2id[starting_text[-1]]] = 1.\n",
    "            # creating test\n",
    "            for i in tqdm(range(options.max_text)):\n",
    "                prediction = test_op.eval({test_data: test_X})[0]\n",
    "                next_char_one_hot = sample(prediction)\n",
    "                next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "                test_generated += next_char\n",
    "                test_X = next_char_one_hot.reshape((1, dataset.len_chars))\n",
    "\n",
    "            #print(buf)\n",
    "            # print(test_generated)\n",
    "            with open(os.path.join(save_path, \"generated.txt\"), \"a\") as file:\n",
    "                file.write(\"\\n{0} Start Test {0}\\n\".format(buf))\n",
    "                file.write(test_generated)\n",
    "                file.write(\"\\n{0} End of Test {0}\\n\".format(buf))\n",
    "                            \n",
    "            saver.save(sess, save_path + '/model', global_step=global_step_)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import ag.logging as log\n",
    "save_path = '/pub/dataset/wiki/log/perm'\n",
    "\n",
    "sess = tf.Session()\n",
    "# new_saver = tf.train.import_meta_graph(os.path.join(save_path, 'model-24.meta'))\n",
    "# starting_text\n",
    "#init graph, load model\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    model = tf.train.latest_checkpoint(save_path)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "# saver = tf.train.Saver()\n",
    "log.info(\"Initilized Session\")\n",
    "# tf.global_variables_initializer().run()\n",
    "model = tf.train.latest_checkpoint(save_path)\n",
    "saver.restore(sess, model)\n",
    "log.info(\"Restored most current training\")\n",
    "# set input variable to generate chars from\n",
    "sess.run(reset_test_state)\n",
    "test_generated = starting_text\n",
    "log.info(\"Loading Starting text {}.\".format(starting_text))\n",
    "#for every char in the input sentennce\n",
    "for i in range(len(starting_text) - 1):\n",
    "    #initialize an empty char store\n",
    "    test_X = np.zeros((1, dataset.len_chars))\n",
    "    #store it in id from\n",
    "    test_X[0, char2id[starting_text[i]]] = 1.\n",
    "    #feed it to model, test_prediction is the output value\n",
    "    _ = sess.run(test_op, feed_dict={test_data: test_X})\n",
    "log.info(\"Test Run.\")\n",
    "\n",
    "#where we store encoded char predictions\n",
    "test_X = np.zeros((1, dataset.len_chars))\n",
    "test_X[0, char2id[starting_text[-1]]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#lets generate 500 characters\n",
    "for i in range(500):\n",
    "    #get each prediction probability\n",
    "    prediction = test_op.eval({test_data: test_X})[0]\n",
    "    #one hot encode it\n",
    "    next_char_one_hot = sample(prediction)\n",
    "    #get the indices of the max values (highest probability)  and convert to char\n",
    "    next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "    #add each char to the output text iteratively\n",
    "    test_generated += next_char\n",
    "    #update the \n",
    "    test_X = next_char_one_hot.reshape((1, dataset.len_chars))\n",
    "\n",
    "print(test_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
