#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Feb 15 23:12:59 2017

@author: eric
"""

import os
os.chdir("/home/eric/git/tf_utilities")
import tkart.model as model
import lib.build_network as net               # Plug all the data into Tensorflow... very carefully...
import lib.process_network as proc            # Actually run the TF machine
import lib.mupen64 as data                    # Start a custom dataset based on the given dataset + special sauce
from lib.ipython import hvass_ipython as ipy  #ipython bonus stuffs
import tensorflow as tf
import numpy as np

from tensorflow.python.framework import dtypes

class options(object):
    def __init__(self, verbose=True, learning_rate=1e-5, batch_size=100, optimizer=1, entropy=1):
        self.verbose = verbose
        #self.model_path = "/home/eric/.local/share/mupen64plus/datasets/mariokart64"
        self.learning_rate = learning_rate
        self.batch_size = batch_size 
        self.optimizer = optimizer
        self.entropy = entropy
        ## CNN options
        self.conv_layers = 5      # of conv layers 2 stock
        self.fc_layers = 5        # of fully connected layers 2 stock     
        self.f_size = 5           # fixed size not file size... silly 5 stock
        self.fc_size = 128        # Max # of elements in FC flatened layers
        self.L2NormConst = 0.001  # duh L2NormCOnst...
        self.save_path = "/home/eric/.local/share/mupen64plus/model/mariokart64/"
        
INPUTDATA_IMG = "/home/eric/.local/share/mupen64plus/datasets/mariokart64/_mariokart64_dataset_12_image.npy"
INPUTDATA_LABEL = "/home/eric/.local/share/mupen64plus/datasets/mariokart64/_mariokart64_dataset_12_label.npy"

CONFIG = options(verbose=False, learning_rate=1e-7,batch_size=42,optimizer=2)

DATASET = data.mupenDataset(INPUTDATA_IMG, INPUTDATA_LABEL, CONFIG)
WERX = net.Build_Mupen64_Network(CONFIG, DATASET)
#data = Data(DATASET)
"""
sess = tf.InteractiveSession()


# Learning Functions
L2NormConst = 0.001 # ??
train_vars = tf.trainable_variables()

loss = tf.reduce_mean(tf.square(tf.sub(model.y_, model.y))) +\
			tf.add_n([tf.nn.l2_loss(v) for v in train_vars]) *\
			L2NormConst


train_step = tf.train.AdamOptimizer(1e-5).minimize(loss)

sess.run(tf.global_variables_initializer())

# Training loop variables
epochs = 2
batch_size = 42
num_samples = DATASET._num_examples
step_size = int(num_samples / batch_size) # this was hardcoded in other examples

for epoch in range(epochs):
    for i in range(step_size):
        batch = DATASET.next_batch(batch_size)

        train_step.run(feed_dict={model.x: batch[0], model.y_: batch[1], model.keep_prob: 0.8}) #! Hardcoded value

        if i%25 == 0:
          loss_value = loss.eval(feed_dict={model.x:batch[0], model.y_: batch[1], model.keep_prob: 1.0})  #! hardcoded value
          print("epoch: {} step: {} loss: {}".format(epoch, epoch * batch_size + i, loss_value))
          
sess.close()
"""